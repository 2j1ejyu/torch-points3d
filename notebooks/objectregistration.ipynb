{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aligning objects in scenes\n",
    "\n",
    "This notebooks explores ways to align objects in a scene and extends that idea to one shot detection. The explored pipeline for one shot detection for a gievn object category C is as follow:\n",
    "\n",
    "- pick a scene\n",
    "- pick a random represent of C that is not in the scene\n",
    "- compute a feature descriptor for all points in the scene and in the object. For that we use a pretrained network on a registration task on 3d match\n",
    "- compute matches between the scene and the object using a symmetry constraint\n",
    "- compute an estimate of scale + rotation + translation using a robust estimator such as RANSAC or Teaser++\n",
    "\n",
    "The first part of the notebook shows how the registration algorithm works while the second part applies that to one shot detection. Initial results show that if the actual object is in the scene we have a very good chance of aligning it properly, however if the object is different then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "import panel as pn\n",
    "import os\n",
    "import os.path as osp\n",
    "from omegaconf import OmegaConf\n",
    "pv.set_plot_theme(\"document\")\n",
    "\n",
    "pn.extension('vtk')\n",
    "os.system('/usr/bin/Xvfb :99 -screen 0 1024x768x24 &')\n",
    "os.environ['DISPLAY'] = ':99'\n",
    "os.environ['PYVISTA_OFF_SCREEN'] = 'True'\n",
    "os.environ['PYVISTA_USE_PANEL'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from plyfile import PlyData\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.segmentation.scannet import Scannet\n",
    "from torch_points3d.datasets.oneshot_detection.scannet import ScannetOneShotDetection\n",
    "from torch_points3d.datasets.segmentation import IGNORE_LABEL\n",
    "from torch_points3d.core.data_transform import GridSampling3D, AddOnes, AddFeatByKey\n",
    "from torch_geometric.transforms import Compose\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_points3d.utils.registration import get_matches, fast_global_registration\n",
    "from torch_points3d.applications.pretrained_api import PretainedRegistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = os.path.dirname(os.getcwd())\n",
    "ONE_SHOT_CLASS=4\n",
    "dataroot = os.path.join(DIR,\"data\",\"scannet-oneshot\")\n",
    "transform = Compose([GridSampling3D(mode='last', size=0.02, quantize_coords=True), AddOnes(), AddFeatByKey(add_to_x=True, feat_name=\"ones\")])\n",
    "dataset = ScannetOneShotDetection(dataroot,transform=transform)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some utilities\n",
    "Utilities for plotting and getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(clouds, together=False, colors=[]):\n",
    "    viewers = []\n",
    "    for i,cloud in enumerate(clouds):\n",
    "        if not together or len(viewers) == 0:\n",
    "            v = pv.Plotter(notebook=True)\n",
    "            viewers.append(v)\n",
    "        if len(colors) > i:\n",
    "            color = colors[i]\n",
    "        else:\n",
    "            color = [0.9, 0.7, 0.1]\n",
    "        v.add_points(cloud.pos.numpy(), color=color)\n",
    "            \n",
    "    pan = [pn.panel(v.ren_win, sizing_mode='scale_both', aspect_ratio=1,orientation_widget=True,) for v in viewers]\n",
    "    if together:\n",
    "        return pan[0]\n",
    "    else:\n",
    "        return pn.Row(*pan)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instances(data, label_idx):\n",
    "    instances = []\n",
    "    unique_instances = torch.unique(data.instance_labels)[-1] + 1\n",
    "    for i in torch.unique(data.instance_labels):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        instance_mask = data.instance_labels == i\n",
    "        label = data.y[instance_mask][-1]\n",
    "        if label == label_idx:\n",
    "            instances.append(Data(pos = data.pos[instance_mask], x = data.x[instance_mask], coords = data.coords[instance_mask]))\n",
    "    return instances    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placing an object in a scene\n",
    "This first section explores the precision of placing an object in a scene when the object is present. This is the simplest one shot detection we can think of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first load a pretrained model for registration\n",
    "# This will log some errors, don't worry it's all good!\n",
    "model = PretainedRegistry.from_pretrained(\"minkowski-registration-3dmatch\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some data from teh dataset and extract a gievn instance of a given class. Here 4 is the bed class\n",
    "d15 =dataset[15]\n",
    "beds = get_instances(d15,4)\n",
    "bed15 = beds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(data):\n",
    "    # For a given data object, computes a feature vector for each point\n",
    "    # using the pretrained registration model. Returns a tensor that contains per point features\n",
    "    batch = Batch.from_data_list([data])\n",
    "    with torch.no_grad():\n",
    "        model.set_input(batch, \"cuda\")\n",
    "        output = model.forward()\n",
    "    return output\n",
    "\n",
    "def register(data, obj):\n",
    "    # Computes the transform that aligns obj in data\n",
    "    # - compute features\n",
    "    # - compute matches using the symetry constraint\n",
    "    # - use fast-global-registration to compute rotation and translation\n",
    "    data_feat = compute_features(data)\n",
    "    obj_feats = compute_features(obj)\n",
    "    matches = get_matches(data_feat, obj_feats, sym=True)\n",
    "    print(\"Number of matches = %i\" %matches.shape[0])\n",
    "    T_est = fast_global_registration(obj.pos[matches[:, 1]],data.pos[matches[:, 0]])\n",
    "    transformed_obj = copy.deepcopy(obj)\n",
    "    transformed_obj.pos= obj.pos @ T_est[:3, :3].T + T_est[:3, 3]\n",
    "    return transformed_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = register(d15, bed15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([d15,transformed], together=True, colors = [[0.9, 0.7, 0.1],[0.1, 0.7, 0.9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below can also be used to visualiase how things where matched:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matches(data,obj, max_lines=100):\n",
    "    d_feats = compute_features(data)\n",
    "    obj_feats = compute_features(obj)\n",
    "    matches = get_matches(d_feats, obj_feats, sym=True)\n",
    "    if matches.shape[0]>max_lines:\n",
    "        perm = torch.randperm(matches.shape[0])\n",
    "        idx = perm[:max_lines]\n",
    "        matches = matches[idx,:]\n",
    "    \n",
    "    v = pv.Plotter(notebook=True)\n",
    "    v.add_points(obj.pos.cpu().numpy())\n",
    "    moved_scan = data.pos.cpu().numpy() + np.asarray([5,0,0])\n",
    "    v.add_points(moved_scan)\n",
    "    for i in range(matches.shape[0]):\n",
    "        lines = []\n",
    "        lines.append(moved_scan[matches[i,0]])\n",
    "        lines.append(obj.pos[matches[i,1]].numpy())\n",
    "        v.add_lines(np.asarray(lines), width=5, color=\"green\")\n",
    "    return pn.panel(v.ren_win, sizing_mode='scale_both', aspect_ratio=1,orientation_widget=True,)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matches(d15, bed15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension for one shot detection\n",
    "Now that we have the basic building block in place let's explore the potential of using such method for one shot detection. As explained in the introduction, the proposed pipeline looks like that:\n",
    "\n",
    "- pick a scene\n",
    "- pick a random represent of C that is not in the scene\n",
    "- compute a feature descriptor for all points in the scene and in the object. For that we use a pretrained network on a registration task on 3d match\n",
    "- compute matches between the scene and the object using a symmetry constraint\n",
    "- compute an estimate of scale + rotation + translation using a robust estimator such as RANSAC or Teaser++\n",
    "- if there are more than N inliers that fit this transform then we consider that have a detection\n",
    "\n",
    "The performance of our detector is tested by using a precision and recall metric, an object is considered as being detected when the overlap between the ground truth and predicted bounded boxes is higher than 25% in miou.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.object_detection.box_data import BoxData\n",
    "from torch_points3d.metrics.oneshottracker import OneShotObjectTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.utils.registration import teaser_pp_registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegistrationResult:\n",
    "    \"\"\"  This class is used to store results from the registration model, \n",
    "    it provides a `get_boxes` that is required by the tracker\n",
    "    \"\"\"\n",
    "    def __init__(self,obj, class_label):\n",
    "        if obj == None:\n",
    "            self.box = None\n",
    "            return\n",
    "        \n",
    "        self.transformed_obj = obj\n",
    "        min_pos, max_pos = torch.min(obj.pos,0)[0],torch.max(obj.pos,0)[0]\n",
    "        xi,yi,zi = min_pos\n",
    "        xm, ym, zm = max_pos\n",
    "        corners = torch.tensor([\n",
    "            [xi,yi,zi], [ xm,yi, zi],[xm, ym, zi],[xi, ym, zi],\n",
    "            [xi,yi,zm], [ xm,yi, zi],[xm, ym, zm],[xi, ym, zm],\n",
    "        ])\n",
    "        self.box = BoxData(class_label, corners, 1)\n",
    "        \n",
    "    def get_boxes(self):\n",
    "        if self.box is not None:\n",
    "            return [[self.box]]\n",
    "        else:\n",
    "            return [[]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegistrationModel(torch.nn.Module):\n",
    "    \"\"\" Wraps the functionalities explored in the previous section into an actual pytorch module. It is \n",
    "    not something that can be trained but provides the nice forward interface. results are exposed through the `get_output`\n",
    "    method so that the tracker can work with that\n",
    "    \"\"\"\n",
    "    def __init__(self,class_label, min_inliers = 10):\n",
    "        super().__init__()\n",
    "        self._min_inliers = min_inliers\n",
    "        self.class_label = class_label\n",
    "        self._model =  PretainedRegistry.from_pretrained(\"minkowski-registration-3dmatch\").cuda()\n",
    "    \n",
    "    def compute_features(self,data):\n",
    "        batch = Batch.from_data_list([data])\n",
    "        with torch.no_grad():\n",
    "            self._model.set_input(batch, \"cuda\")\n",
    "            output = self._model.forward()\n",
    "        return output\n",
    "\n",
    "    def forward(self, data,  one_instance):\n",
    "        data_feat = self.compute_features(data)\n",
    "        obj_feats = self.compute_features(one_instance)\n",
    "        matches = get_matches(data_feat, obj_feats, sym=True)\n",
    "\n",
    "        # T_est = fast_global_registration(one_instance.pos[matches[:, 1]],data.pos[matches[:, 0]])\n",
    "        T_est, inliers = teaser_pp_registration(one_instance.pos[matches[:, 1]],data.pos[matches[:, 0]])\n",
    "        if len(inliers) > self._min_inliers:\n",
    "            transformed_obj = copy.deepcopy(one_instance)\n",
    "            transformed_obj.pos= one_instance.pos @ T_est[:3, :3].T + T_est[:3, 3]\n",
    "            self.output = RegistrationResult(transformed_obj, self.class_label)\n",
    "        else:\n",
    "            self.output = RegistrationResult(None, self.class_label)\n",
    "        \n",
    "    def get_output(self):\n",
    "        return self.output\n",
    "    \n",
    "    def get_current_losses(self):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialise our inference pipeline by picking a category and one object within this category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONE_SHOT_CLASS = 4\n",
    "d0 = dataset[0]\n",
    "bed0 = get_instances(d0, ONE_SHOT_CLASS)[0]\n",
    "model = RegistrationModel(dataset.NYU40ID2CLASS[ONE_SHOT_CLASS],min_inliers = 10)\n",
    "plot([d0,bed0], together=True, colors = [[0.9, 0.7, 0.1],[0.1, 0.7, 0.9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the inference loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "max_iter = 2\n",
    "tracker = OneShotObjectTracker(dataset)\n",
    "detected = []\n",
    "with tqdm.notebook.tqdm(dataset) as bar:\n",
    "    for i,d in enumerate(bar):\n",
    "        if i > max_iter:\n",
    "            break\n",
    "        beds = get_instances(d, ONE_SHOT_CLASS)\n",
    "        if not len(beds):\n",
    "            continue\n",
    "        model(d, bed0)\n",
    "        out = model.get_output()\n",
    "        if len(out.get_boxes()[0]):\n",
    "            detected.append((d,out.transformed_obj))\n",
    "        tracker.track(model, Batch.from_data_list([d]))\n",
    "        count += 1\n",
    "        bar.set_postfix(**tracker.get_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker._tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker._ngt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
